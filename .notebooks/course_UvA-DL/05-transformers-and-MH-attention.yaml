title: 'Tutorial 5: Transformers and Multi-Head Attention'
author: Phillip Lippe
created: 2021-06-30
license: CC BY-SA
tags:
- Text
description: 'In this tutorial, we will discuss one of the most impactful architectures
  of the last 2 years: the Transformer model.

  Since the paper Attention Is All You Need by Vaswani et al. had been published in
  2017,

  the Transformer architecture has continued to beat benchmarks in many domains, most
  importantly in Natural Language Processing.

  Transformers with an incredible amount of parameters can generate long, convincing
  essays, and opened up new application fields of AI.

  As the hype of the Transformer architecture seems not to come to an end in the next
  years,

  it is important to understand how it works, and have implemented it yourself, which
  we will do in this notebook.

  This notebook is part of a lecture series on Deep Learning at the University of
  Amsterdam.

  The full list of tutorials can be found at https://uvadlc-notebooks.rtfd.io.

  '
accelerator:
- GPU
OS: Linux
python: 3.10.12
environment:
- pytorch-lightning==2.4.0
- torchvision==0.19.1+cu121
- numpy==2.0.2
- torchmetrics==1.4.1
- seaborn==0.13.2
- matplotlib==3.9.2
- torch==2.4.1+cu121
published: '2025-04-03T19:16:52.842963'
